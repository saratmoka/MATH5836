{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models: Logistic vs 1-Layer vs 3-Layer Neural Networks vs Random Forest\n",
    "**Dataset**: Wine Quality (UCI)  \n",
    "**Dataset Details**:  \n",
    "- **Source**: UCI Machine Learning Repository (Red Wine Variants)  \n",
    "- **Samples**: 1,599 red wines with 11 physicochemical features  \n",
    "- **Features**: Fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol  \n",
    "- **Target**: Binary quality classification (0=low quality ≤5, 1=high quality >5)  \n",
    "\n",
    "Physical-chemical properties interact nonlinearly to determine quality. Simple logistic regression assumes linear feature relationships, while neural networks and random forests can model complex interactions.\n",
    "\n",
    "Progress cell-by-cell. Execute each cell where code is already written.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Suppress only ConvergenceWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Load Wine Quality Dataset\n",
    "\n",
    "- Load dataset from UCI URL\n",
    "- Inspect class distribution\n",
    "- Separate features (X) and labels (y)\n",
    "- Show `df.head()`, shapes, and correlation matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "df = pd.read_csv(url, delimiter=';')\n",
    "X = df.drop('quality', axis=1).values\n",
    "y = df['quality'].apply(lambda x: 1 if x > 5 else 0).values  # Binary\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(\"Label distribution:\", np.unique(y, return_counts=True))\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Train/Test Split & Normalization\n",
    "\n",
    "- Split data into 70% train / 30% test\n",
    "- Apply standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Model Definitions\n",
    "\n",
    "- Initialize Logistic Regression\n",
    "- Initialize 1-layer MLP (32 neurons)\n",
    "- Initialize 3-layer MLP (128,64,32)\n",
    "- Initialize Random Forest (100 trees, OOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic\n",
    "log_reg = LogisticRegression(\n",
    "    tol=1e-4, max_iter=10000, random_state=42)\n",
    "\n",
    "# 1-layer NN\n",
    "mlp_1layer = MLPClassifier(\n",
    "    hidden_layer_sizes=(32,), activation='relu',\n",
    "    solver='adam', tol=1e-4, max_iter=10000,\n",
    "    random_state=42)\n",
    "\n",
    "# 3-layer NN\n",
    "mlp_3layer = MLPClassifier(\n",
    "    hidden_layer_sizes=(128,64,32), activation='relu',\n",
    "    solver='adam', tol=1e-4, max_iter=10000,\n",
    "    random_state=42)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100, oob_score=True,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Train & Evaluate Once\n",
    "\n",
    "- Fit each model on training data\n",
    "- Predict on test set\n",
    "- Compute and print test accuracy (and OOB for RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "log_reg.fit(X_train, y_train)\n",
    "mlp_1layer.fit(X_train, y_train)\n",
    "mlp_3layer.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_log = log_reg.predict(X_test)\n",
    "y_mlp1 = mlp_1layer.predict(X_test)\n",
    "y_mlp3 = mlp_3layer.predict(X_test)\n",
    "y_rf   = rf.predict(X_test)\n",
    "\n",
    "# Accuracies\n",
    "acc_log  = accuracy_score(y_test, y_log)\n",
    "acc_mlp1 = accuracy_score(y_test, y_mlp1)\n",
    "acc_mlp3 = accuracy_score(y_test, y_mlp3)\n",
    "acc_rf   = accuracy_score(y_test, y_rf)\n",
    "\n",
    "print(f\"Logistic  Test  Acc     : {acc_log:.3f}\")\n",
    "print(f\"MLP 1-layer Test Acc    : {acc_mlp1:.3f}\")\n",
    "print(f\"MLP 3-layer Test Acc    : {acc_mlp3:.3f}\")\n",
    "print(f\"Random Forest Test Acc  : {acc_rf:.3f}\")\n",
    "print(f\"Random Forest OOB Score : {rf.oob_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Feature Importances (Random Forest)\n",
    "Inspect the top-5 features by mean decrease in impurity from the RF model trained above. Feature importance is a way to quantify how much each input feature contributes to a model’s predictions.  In the context of tree-based models (like random forests), one common arroadch is impurity-based (a.k.a. “Gini” or “Mean Decrease in Impurity”):\n",
    "\n",
    "- Every time a tree node splits on feature j, the impurity (Gini or entropy for classification, variance for regression) is reduced.\n",
    "- You sum those impurity reductions over all nodes in all trees where j is used, then normalize.\n",
    "- Features with large total impurity reduction are deemed more “important.”\n",
    "\n",
    "We use the RandomForestClassifier’s built-in attribute `feature_importances_` to measure this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the last trained rf_\n",
    "importances = rf.feature_importances_\n",
    "feat_names = df.columns[:-1]\n",
    "imp_df = pd.DataFrame({\n",
    "    'feature': feat_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(imp_df.head(5))\n",
    "sns.barplot(x='importance', y='feature', data=imp_df.head(5))\n",
    "plt.title('Top-5 Feature Importances (RF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Mean Performance Over 10 Experiments\n",
    "\n",
    "- Repeat split, train, eval with seeds 0–9\n",
    "- Report mean test accuracy for all four models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_log_list  = []\n",
    "acc_mlp1_list = []\n",
    "acc_mlp3_list = []\n",
    "acc_rf_list   = []\n",
    "\n",
    "for seed in range(10):\n",
    "    # split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "    # scale\n",
    "    scaler = StandardScaler().fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_te = scaler.transform(X_te)\n",
    "\n",
    "    # models\n",
    "    lr = LogisticRegression(max_iter=5000, tol=1e-4, random_state=seed)\n",
    "    m1 = MLPClassifier(hidden_layer_sizes=(32,), max_iter=100000,\n",
    "                      tol=1e-4, random_state=seed)\n",
    "    m3 = MLPClassifier(hidden_layer_sizes=(128,64,32), max_iter=100000,\n",
    "                      tol=1e-4, random_state=seed)\n",
    "    rf_ = RandomForestClassifier(\n",
    "        n_estimators=100, oob_score=False,\n",
    "        random_state=seed)\n",
    "\n",
    "    # fit\n",
    "    lr.fit(X_tr, y_tr)\n",
    "    m1.fit(X_tr, y_tr)\n",
    "    m3.fit(X_tr, y_tr)\n",
    "    rf_.fit(X_tr, y_tr)\n",
    "\n",
    "    # predict\n",
    "    a_lr = accuracy_score(y_te, lr.predict(X_te))\n",
    "    a_m1 = accuracy_score(y_te, m1.predict(X_te))\n",
    "    a_m3 = accuracy_score(y_te, m3.predict(X_te))\n",
    "    a_rf = accuracy_score(y_te, rf_.predict(X_te))\n",
    "\n",
    "    acc_log_list.append(a_lr)\n",
    "    acc_mlp1_list.append(a_m1)\n",
    "    acc_mlp3_list.append(a_m3)\n",
    "    acc_rf_list.append(a_rf)\n",
    "\n",
    "print(\"\\nMean Accuracies over 10 runs:\")\n",
    "print(f\"Logistic    : {np.mean(acc_log_list):.3f}\")\n",
    "print(f\"MLP 1-layer : {np.mean(acc_mlp1_list):.3f}\")\n",
    "print(f\"MLP 3-layer : {np.mean(acc_mlp3_list):.3f}\")\n",
    "print(f\"RandomForest: {np.mean(acc_rf_list):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ ROC and AUC\n",
    "\n",
    "- Plot ROC Curves & compute AUC for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Dictionary of fitted models\n",
    "models = {\n",
    "    'Logistic Regression': log_reg,\n",
    "    'MLP 1-layer': mlp_1layer,\n",
    "    'MLP 3-layer': mlp_3layer,\n",
    "    'Random Forest': rf\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for name, model in models.items():\n",
    "    # get probability estimates for the positive class\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    # compute ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    # compute AUC\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    # plot\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{name} (AUC = {auc:.3f})\")\n",
    "\n",
    "# plot the random-chance diagonal\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1, label='Chance')\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves for Wine Quality Classifiers\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
