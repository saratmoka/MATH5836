{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models: Logistic vs 1-Layer vs 3-Layer Neural Networks\n",
    "**Dataset**: Wine Quality (UCI)  \n",
    "**Dataset Details**:  \n",
    "- **Source**: UCI Machine Learning Repository (Red Wine Variants)  \n",
    "- **Samples**: 1,599 red wines with 11 physicochemical features  \n",
    "- **Features**: Fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol  \n",
    "- **Target**: Binary quality classification (0=low quality ≤5, 1=high quality >5)  \n",
    "\n",
    "**Challenge**:  \n",
    "Physical-chemical properties interact nonlinearly to determine quality. Simple logistic regression assumes linear feature relationships, while neural networks (with hidden layers) can model complex interactions through activation functions like ReLU.\n",
    "\n",
    "<h1 style=\"color:red;\">Intructions</h1>\n",
    "\n",
    "- Progress cell-by-cell.\n",
    "- Check for **<a style=\"color:red;\">Execute</a>s**, where codes for <a style=\"color:green;\">green</a> tasks are already written and you are expected write codes to excute the remaining tasks.\n",
    "- After completing all the coding tasks, check **<a style=\"color:red;\">Compute</a>** challenge at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Suppress only ConvergenceWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Load Wine Quality Dataset\n",
    "<h3 style=\"color:red;\">Execute:</h3>\n",
    "\n",
    "- <a style=\"color:green;\">Load dataset from UCI URL</a>\n",
    "- <a style=\"color:green;\">Inspect class distribution</a>\n",
    "- <a style=\"color:green;\">Separate features (X) and labels (y)</a>\n",
    "- Using `df.head()`, check the column names and first few rows of the data\n",
    "- Print the shape of X and y and compare with the details provided in the above data description\n",
    "- Print the correlation matrix (you can use the code from the previous tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "df = pd.read_csv(url, delimiter=';')\n",
    "X = df.drop('quality', axis=1).values\n",
    "y = df['quality'].apply(lambda x: 1 if x > 5 else 0).values  # Binary classification\n",
    "\n",
    "# Print head of the dataframe\n",
    "\n",
    "\n",
    "# print shapes of X and y\n",
    "\n",
    "\n",
    "# print corr matrix\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Train/Test Split & Normalization\n",
    "<h3 style=\"color:red;\">Execute:</h3>\n",
    "\n",
    "- Split data into 70% train / 30% test\n",
    "- <a style=\"color:green;\">Apply standardization</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for splitting the data into (X_train, y_train) and (X_test, y_test)\n",
    "\n",
    "\n",
    "# apply standardization \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Model Definitions, Training & Evaluation\n",
    "<h3 style=\"color:red;\">Execute:</h3>\n",
    "\n",
    "- <a style=\"color:green;\">Initialize Logistic Regression</a>\n",
    "- <a style=\"color:green;\">Initialize MLP with 1 hidden layer (50 neurons)</a>\n",
    "- <a style=\"color:green;\">Initialize MLP with 2 hidden layers (50,25 neurons)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "log_reg = LogisticRegression(tol=1e-4, \n",
    "                             max_iter=10000, \n",
    "                             random_state=42)\n",
    "# 1-layer NN classifier\n",
    "mlp_1layer = MLPClassifier(hidden_layer_sizes=(32,), \n",
    "                           solver='adam', \n",
    "                           activation='relu',\n",
    "                           tol=1e-4, \n",
    "                           max_iter=10000, \n",
    "                           random_state=42)\n",
    "# 3-layer NN classifier\n",
    "mlp_3layer = MLPClassifier(hidden_layer_sizes=(128, 64, 32), \n",
    "                           solver='adam', \n",
    "                           activation='relu',\n",
    "                           tol=1e-4, \n",
    "                           max_iter=10000, \n",
    "                           random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">Execute:</h3>\n",
    "\n",
    "- <a style=\"color:green;\">Fit models on training data</a>\n",
    "- For each model, get predictions\n",
    "- Compute test accuracy for each for each model using true labels and predicted labels\n",
    "- Print the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_train, y_train)\n",
    "mlp_1layer.fit(X_train, y_train)\n",
    "mlp_3layer.fit(X_train, y_train)\n",
    "\n",
    "# get predictions\n",
    "\n",
    "\n",
    "# Compute accuracy for each model\n",
    "\n",
    "\n",
    "# print the accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Mean Performance Over 10 experiments\n",
    "<h3 style=\"color:red;\">Execute:</h3>\n",
    "\n",
    "- Combine above tasks to compute the average performance by running 10 experimenst with different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_log_list = []\n",
    "acc_mlp1_list = []\n",
    "acc_mlp3_list = []\n",
    "\n",
    "for seed in range(10):\n",
    "\n",
    "    # Split the data\n",
    "    \n",
    "\n",
    "    # Scale the data\n",
    "    \n",
    "\n",
    "    \n",
    "    # Logistic regression model\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # 1-layer NN classifier\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # 3-layer NN classifier\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # fit the models\n",
    "\n",
    "    \n",
    "\n",
    "    # get predictions\n",
    "\n",
    "    \n",
    "\n",
    "    # Compute accuracy for each model\n",
    "\n",
    "\n",
    "    \n",
    "    # print the accuracies for each experiment\n",
    "\n",
    "    \n",
    "\n",
    "    # collect the accuracies\n",
    "\n",
    "\n",
    "# print mean accuracies \n",
    "print(\"\\n-------- Final mean results ---------\")\n",
    "print(\"Logistic Regression Mean Test Accuracy: \", np.mean(acc_log_list))\n",
    "print(\"1-Layer MLP Mean Test Accuracy: \", np.mean(acc_mlp1_list))\n",
    "print(\"3-Layer MLP Mean Test Accuracy: \", np.mean(acc_mlp3_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Parameter Count Challenge\n",
    "<h3 style=\"color:red;\">Compute the number of parameters:</h3>\n",
    "\n",
    "Compute the number of learnable paramters in each one of the above three models. **Note** that `MLPClassifier` uses `sigmoid` function instead of `softmax` for binary classification.\n",
    "\n",
    "- *Logistic Regression:*  \n",
    "\n",
    " \n",
    "- *Above 1-Layer MLP:*\n",
    "\n",
    "  \n",
    "- *Above 3-Layer MLP:*\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
