{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Linear Regression on a Synthetic Dataset\n",
    "\n",
    "1. Generate synthetic data with an intercept and 5 features (100 samples).\n",
    "2. Implement a gradient descent solver.\n",
    "3. Compute solutions by closed‐form OLS, scikit‐learn, and gradient descent.\n",
    "4. Compare the parameter estimates to verify they coincide.\n",
    "5. Feature–Response correlation and $R^2$ on Training Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 100\n",
    "n_features = 5\n",
    "\n",
    "\"\"\"\n",
    "True parameters (intercept + 5 coefficients)\n",
    "\"\"\"\n",
    "beta_true = np.array([2.0, -1.5, 3.0, 0.0, 1.0, -2.0])\n",
    "\n",
    "# Design matrix: first column of ones, then random features\n",
    "X_feat = np.random.randn(n_samples, n_features)\n",
    "X = np.hstack([np.ones((n_samples,1)), X_feat])  # shape (100,6)\n",
    "\n",
    "# Generate targets with Gaussian noise\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y = X.dot(beta_true) + noise\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"\\nFirst 5 rows of X:\\n\", X[:5])\n",
    "print(\"\\nFirst 5 targets y:\\n\", y[:5])\n",
    "print(\"\\nTrue coefficients:\\n\", beta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, lr=0.01, n_iters=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    Solve for theta in linear regression by batch gradient descent.\n",
    "    X: design matrix (n x d), y: targets (n, )\n",
    "    lr: learning rate, n_iters: number of iterations\n",
    "    Returns theta (d,)\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    theta = np.zeros(d)\n",
    "    for it in range(n_iters):\n",
    "        grad = (X.T.dot(X.dot(theta) - y)) / n\n",
    "        theta -= lr * grad\n",
    "        if verbose and it % (n_iters//5) == 0:\n",
    "            loss = np.mean((X.dot(theta) - y)**2)\n",
    "            print(f\"Iter {it:4d}, MSE={loss:.4f}\")\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Compute Solutions: OLS, scikit‐learn, and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a) Closed‐form OLS solution\n",
    "beta_closed = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "# 3b) scikit‐learn solution\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_model = LinearRegression(fit_intercept=False)\n",
    "lr_model.fit(X, y)\n",
    "beta_sklearn = lr_model.coef_\n",
    "\n",
    "# 3c) Gradient Descent solution\n",
    "beta_gd = gradient_descent(X, y, lr=0.1, n_iters=5000)\n",
    "\n",
    "print(\"True theta:        \", beta_true)\n",
    "print(\"Closed‐form theta: \", np.round(beta_closed, 4))\n",
    "print(\"sklearn theta:     \", np.round(beta_sklearn, 4))\n",
    "print(\"GD solution theta: \", np.round(beta_gd, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Comparison and Discussion\n",
    "- The parameter estimates from closed‐form OLS, scikit‐learn, and gradient descent should all be very close (or identical) to each other.\n",
    "- They shoudl be close to the true `beta_true`.\n",
    "- Minor differences arise from noise and finite iterations in GD.\n",
    "- If GD hasn't converged, adjust the learning rate `lr` or increase `n_iters`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Feature–Response Correlation and $R^2$ on Training Data\n",
    "Compute the Pearson correlation between each feature (excluding the intercept) and the predicted response, and evaluate the coefficient of determination $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 5a) Predicted response via gradient descent solution\n",
    "y_pred = X.dot(beta_gd)\n",
    "\n",
    "# 5b) Pearson correlation for each feature (skip intercept at col 0)\n",
    "print(\"Feature and y_pred correlations:\")\n",
    "for j in range(1, X.shape[1]):\n",
    "    corr = np.corrcoef(X[:, j], y_pred)[0, 1]\n",
    "    print(f\"\\tFeature {j}: corr = {corr:.4f}\")\n",
    "\n",
    "# 5c) R^2 score on training data\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(f\"\\nR^2 on training set: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
