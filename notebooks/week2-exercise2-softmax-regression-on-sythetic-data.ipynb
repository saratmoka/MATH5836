{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Softmax Regression on a Synthetic Classification Dataset\n",
    "\n",
    "1. Generate synthetic classification data with 3 classes and 5 features (100 samples).\n",
    "2. Train a softmax regression model using scikit-learn.\n",
    "3. Evaluate the model using accuracy and confusion matrix.\n",
    "4. Visualize decision boundary (for 2D projection).\n",
    "5. Discuss feature importance and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Generate Synthetic Classification Data\n",
    "`make_blobs` from `sklearn.datasets` generates synthetic data; see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate classification data with 3 classes using make_blobs\n",
    "X, y = make_blobs(n_samples=100, centers=3, n_features=5, cluster_std=1.5, random_state=0)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"First 5 samples of X:\\n\", X[:5])\n",
    "print(\"First 5 labels y:\\n\", y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Train Softmax Regression Model\n",
    "Softmax in `sklearn` is part of the `LogisticRegression` since softmax regression is also known as multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train model using 'lbfgs' optimizer\n",
    "clf = LogisticRegression(solver='saga', tol=1e-3, max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model coefficients:\", clf.coef_)\n",
    "print(\"Intercept:\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Visualize Decision Boundary (2D Projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_vis = pca.fit_transform(X)\n",
    "\n",
    "clf_vis = LogisticRegression(solver='saga', tol=1e-3, max_iter=1000).fit(X_vis, y)\n",
    "\n",
    "# Plot decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(X_vis[:,0].min(), X_vis[:,0].max(), 100),\n",
    "                     np.linspace(X_vis[:,1].min(), X_vis[:,1].max(), 100))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = clf_vis.predict_proba(grid).reshape(xx.shape + (3,))\n",
    "\n",
    "plt.contourf(xx, yy, probs[:, :, 0], 25, cmap=\"RdBu\", alpha=0.8)\n",
    "plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y, cmap=\"RdBu\", edgecolors='k')\n",
    "plt.title(\"Softmax Regression Decision Boundary (PCA Projection)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Feature Importance and Discussion\n",
    "- Coefficients indicate feature importance.\n",
    "- Consider regularization or feature selection for high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
