{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Classification Loss Metrics\n",
    "\n",
    "Compute the following losses on a synthetic binary classification dataset:\n",
    "- Log‐Loss (Cross‐Entropy)\n",
    "- Exponential Loss (useful later in adaboost algorithm)\n",
    "- Hinge Loss\n",
    "\n",
    "Use both custom implementations and scikit‐learn where available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports and data generation\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, hinge_loss\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 200\n",
    "\n",
    "# true binary labels {0,1}\n",
    "y = np.random.randint(0, 2, size=n)\n",
    "\n",
    "# random predicted probabilities in (0,1)\n",
    "eps = 1e-15\n",
    "p_hat = np.clip(np.random.rand(n), eps, 1 - eps)\n",
    "\n",
    "# decision function f(x) = log(p/(1-p)) for hinge/exponential\n",
    "f_hat = np.log(p_hat / (1 - p_hat))\n",
    "\n",
    "# convert labels to ±1 for hinge/exponential\n",
    "y_signed = 2 * y - 1\n",
    "\n",
    "print(\"First 5 true labels:\", y[:5])\n",
    "print(\"First 5 probabilities:\", np.round(p_hat[:5], 3))\n",
    "print(\"First 5 decisions f_hat:\", np.round(f_hat[:5], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_log_loss(y_true, p_pred):\n",
    "    \"\"\"\n",
    "    Binary cross‐entropy:\n",
    "      −(y·log p + (1−y)·log(1−p))\n",
    "    \"\"\"\n",
    "    return -np.mean(y_true * np.log(p_pred) + (1 - y_true) * np.log(1 - p_pred))\n",
    "\n",
    "def custom_exponential_loss(y_s, f):\n",
    "    \"\"\"\n",
    "    AdaBoost exponential loss: exp(−y·f)\n",
    "    \"\"\"\n",
    "    return np.mean(np.exp(- y_s * f))\n",
    "\n",
    "def custom_hinge_loss(y_s, f):\n",
    "    \"\"\"\n",
    "    Hinge loss: max(0, 1 − y·f)\n",
    "    \"\"\"\n",
    "    return np.mean(np.maximum(0, 1 - y_s * f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Compute and Compare Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom implementations\n",
    "log_c   = custom_log_loss(y, p_hat)\n",
    "exp_c   = custom_exponential_loss(y_signed, f_hat)\n",
    "hinge_c = custom_hinge_loss(y_signed, f_hat)\n",
    "\n",
    "# sklearn implementations\n",
    "log_s   = log_loss(y, p_hat)\n",
    "hinge_s = hinge_loss(y, f_hat, labels=[-1,1])\n",
    "\n",
    "print(f\"Custom Log‐Loss:    {log_c:.4f}\")\n",
    "print(f\"sklearn Log‐Loss:   {log_s:.4f}\\n\")\n",
    "print(f\"Custom Exponential: {exp_c:.4f}\")\n",
    "print(\"(No direct sklearn Exponential loss)\",\"\\n\")\n",
    "print(f\"Custom Hinge Loss:  {hinge_c:.4f}\")\n",
    "print(f\"sklearn Hinge Loss: {hinge_s:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "- The custom log‐loss matches `sklearn.metrics.log_loss`.\n",
    "- Exponential loss offers an alternative boosting‐style penalty.\n",
    "- The custom hinge loss aligns with `sklearn.metrics.hinge_loss` when using ±1 labels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
