{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.1: Boosting on Pima Indian Diabetes Dataset\n",
    "\n",
    "**Objective:** Compare boosting algorithms (AdaBoost vs GradientBoosting) on the Pima Indian Diabetes classification task.\n",
    "\n",
    "## Experiment Setup\n",
    "- **Dataset:** Pima Indian Diabetes (8 features, binary target)  \n",
    "- **Test Size:** 30% holdout  \n",
    "- **Metrics:** Accuracy, ROC AUC  \n",
    "- **Models:**  \n",
    "  1. AdaBoostClassifier   \n",
    "  2. GradientBoostingClassifier (loss='deviance')  \n",
    "- **Stability:** 10 independent runs for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Data Loading & Inspection\n",
    "\n",
    "Load Pima Indian Diabetes from a CSV URL, assign column names, and inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL from UCI repository mirror\n",
    "url = (\n",
    "    'https://raw.githubusercontent.com/jbrownlee/Datasets/master/'\n",
    "    'pima-indians-diabetes.csv'\n",
    ")\n",
    "cols = [\n",
    "    'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
    "    'Insulin', 'BMI', 'DiabetesPedigree', 'Age', 'Outcome'\n",
    "]\n",
    "df = pd.read_csv(url, header=None, names=cols)\n",
    "\n",
    "print(df.head())\n",
    "print(\"\\nShape:\", df.shape)\n",
    "print(\"Class counts:\", df['Outcome'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Train/Test Split & Normalization\n",
    "\n",
    "- Split into 70% train / 30% test  \n",
    "- Standardize features (zero mean, unit variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Outcome', axis=1).values\n",
    "y = df['Outcome'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Model Definitions\n",
    "\n",
    "Initialize AdaBoost and GradientBoosting classifiers with default settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbc = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(ada)\n",
    "print(gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Single‐Run Training & Evaluation\n",
    "\n",
    "Fit each model once and report accuracy, ROC AUC, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "ada.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_ada = ada.predict(X_test)\n",
    "y_gbc = gbc.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "acc_ada = accuracy_score(y_test, y_ada)\n",
    "acc_gbc = accuracy_score(y_test, y_gbc)\n",
    "auc_ada = roc_auc_score(y_test, ada.predict_proba(X_test)[:,1])\n",
    "auc_gbc = roc_auc_score(y_test, gbc.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(f\"AdaBoost    | Acc: {acc_ada:.3f}, AUC: {auc_ada:.3f}\")\n",
    "print(f\"GradBoost   | Acc: {acc_gbc:.3f}, AUC: {auc_gbc:.3f}\")\n",
    "print(\"\\nAdaBoost Confusion Matrix:\\n\", confusion_matrix(y_test, y_ada))\n",
    "print(\"\\nGradBoost Confusion Matrix:\\n\", confusion_matrix(y_test, y_gbc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Mean Performance Over 10 Runs\n",
    "\n",
    "Repeat train/test split, fit, and evaluate 10 times with different seeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for seed in range(10):\n",
    "    # split & scale\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "    sc = StandardScaler().fit(X_tr)\n",
    "    X_tr, X_te = sc.transform(X_tr), sc.transform(X_te)\n",
    "    \n",
    "    # init\n",
    "    m1 = AdaBoostClassifier(\n",
    "        n_estimators=100, \n",
    "        learning_rate=0.1, \n",
    "        random_state=seed)\n",
    "    m2 = GradientBoostingClassifier(\n",
    "        n_estimators=100, \n",
    "        learning_rate=0.1,\n",
    "        max_depth=3, \n",
    "        random_state=seed)\n",
    "    \n",
    "    # fit\n",
    "    m1.fit(X_tr, y_tr)\n",
    "    m2.fit(X_tr, y_tr)\n",
    "    \n",
    "    # eval\n",
    "    a1 = accuracy_score(y_te, m1.predict(X_te))\n",
    "    a2 = accuracy_score(y_te, m2.predict(X_te))\n",
    "    auc1 = roc_auc_score(y_te, m1.predict_proba(X_te)[:,1])\n",
    "    auc2 = roc_auc_score(y_te, m2.predict_proba(X_te)[:,1])\n",
    "    results.append((a1, auc1, a2, auc2))\n",
    "\n",
    "arr = np.array(results)\n",
    "print(\"AdaBoost   Mean Acc: %.3f ± %.3f | Mean AUC: %.3f ± %.3f\" % (\n",
    "    arr[:,0].mean(), arr[:,0].std(), arr[:,1].mean(), arr[:,1].std()))\n",
    "print(\"GradBoost  Mean Acc: %.3f ± %.3f | Mean AUC: %.3f ± %.3f\" % (\n",
    "    arr[:,2].mean(), arr[:,2].std(), arr[:,3].mean(), arr[:,3].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ ROC Curves\n",
    "\n",
    "Plot ROC curves of both models on one representative split (seed=42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "sc = StandardScaler().fit(X_tr)\n",
    "X_tr, X_te = sc.transform(X_tr), sc.transform(X_te)\n",
    "\n",
    "ada.fit(X_tr, y_tr)\n",
    "gbc.fit(X_tr, y_tr)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for name, model in [('AdaBoost', ada), ('GradBoost', gbc)]:\n",
    "    proba = model.predict_proba(X_te)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_te, proba)\n",
    "    auc = roc_auc_score(y_te, proba)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{name} (AUC={auc:.3f})\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Feature Importances (GBC)\n",
    "\n",
    "Show impurity‐based importances from GradientBoosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impurity-based\n",
    "imp = gbc.feature_importances_\n",
    "fi = pd.DataFrame({'feature': df.columns[:-1], 'impurity': imp})\n",
    "fi = fi.sort_values('impurity', ascending=False)\n",
    "print(fi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ Challenges\n",
    "1. **Grid‐search** over `n_estimators` and `learning_rate` for both AdaBoost and GBC using `GridSearchCV`.  \n",
    "2. **Try a different dataset** (e.g. Energy or Abalone) and repeat the boosting comparisons.  \n",
    "3. **Loss variants:** for `GradientBoostingClassifier`, swap `loss='exponential'` and compare performance.  \n",
    "4. **Calibration:** use `CalibratedClassifierCV` to calibrate probabilities and see if AUC improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
